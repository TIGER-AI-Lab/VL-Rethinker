<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description" content="VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning">
    <meta property="og:title" content="VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning" />
    <meta property="og:description" content="We propose an approach to incentivize mutlimodal reasoning capabilities" />
    <meta property="og:url" content="https://github.com/TIGER-AI-Lab/VL-Rethinker/" />
    <meta property="og:image" content="" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <title>VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning</title>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">
                            VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning
                        </h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://haozheh3.github.io/" style="text-decoration: none; color: inherit;">Haozhe Wang</a>,
                            </span>
                            <span class="author-block">
                                Chao Qu,
                            </span>
                            <span class="author-block">
                                Zuming Huang,
                            </span>
                            <span class="author-block">
                                Wei Chu,
                            </span>
                            <span class="author-block">
                                Fangzhen Lin,
                            </span>
                            <span class="author-block">
                                <a href="https://wenhuchen.github.io/" style="text-decoration: none; color: inherit;">Wenhu Chen</a>,
                            </span>
                        </div>

                        

                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                HKUST, INF.AI, University of Waterloo
                            </span>
                            <br>
                            <span class="author-block">Corresponding to:</span>
                            <span class="author-block"><a href="mailto:yiming.jia@mail.utoronto.ca">jasper.whz@outlook.com</a>,</span>
                            <span class="author-block"><a href="mailto:wenhuchen@uwaterloo.ca">wenhuchen@uwaterloo.ca</a></span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- GitHub link -->
                                <span class="link-block">
                                    <a href="https://github.com/TIGER-AI-Lab/VL-Rethinker/" target="_blank"
                                    class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="fab fa-github"></i>
                                    </span>
                                    <span>Code</span>
                                    </a>
                                </span>

                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2504.08837" target="_blank"
                                    class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="ai ai-arxiv"></i>
                                    </span>
                                    <span>Paper</span>
                                    </a>
                                </span>

                                <span class="link-block">
                                  <a href="https://huggingface.co/TIGER-Lab/VL-Rethinker-7B" target="_blank"
                                  class="external-link button is-normal is-rounded is-dark">
                                      <span class="icon">
                                        ðŸ¤—
                                      </span>
                                      <span>VL-Rethinker-7B</span>
                                  </a>
                                </span>

                                <span class="link-block">
                                  <a href="https://huggingface.co/TIGER-Lab/VL-Rethinker-72B" target="_blank"
                                  class="external-link button is-normal is-rounded is-dark">
                                      <span class="icon">
                                        ðŸ¤—
                                      </span>
                                      <span>VL-Rethinker-72B</span>
                                  </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container" style="margin-bottom: 2vh;margin-top: -6vh;">
          <!-- Abstract. -->
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">ðŸ””News</h2>
              <div class="content has-text-justified">
                <p>
                    <b>ðŸ”¥[2025-04] Our paper <a href="https://arxiv.org/abs/2504.08837">VL-Rethinker</a> and <a href="https://huggingface.co/collections/TIGER-Lab/vl-rethinker-67fdc54de07c90e9c6c69d09">Models</a> are out ðŸš€. We are actively working on releasing the training code and training queries!</b>
                </p>
              </div>
              <h2 class="title is-3">Introduction</h2>
              <div class="content has-text-justified">
                <p>
                    Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. However, their multimodal reasoning capabilities remain on par with fast-thinking models. <u>In this paper, we aim to enhance the slow-thinking capabilities of vision-language models using reinforcement learning (without relying on distillation) to advance the state of the art. </u>
                </p>
              </div>
            </div>
          </div>
          <!--/ Abstract. -->
      </div>
    </section>

    <section class="hero is-light is-small">
        <div class="hero-body has-text-centered">
          <h1 class="title is-1 acecoder">
            <span class="acecoder">VL-Rethinker</span>
          </h1>
        </div>
    </section>

    <section class="section">
        <div class="container">
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Overview</h2>
              <div class="content has-text-justified">
                <p>We identified a critical limitation of GRPO especially for 72B training: <u>the vanishing advantages problem</u>. To mitigate this issue, we introduced a novel technique called <u>Selective Sample Replay (SSR)</u>. While this approach yields strong performance, the resulting RL-trained models exhibit limited self-reflection or self-verification. To further encourage slow-thinking, we introduce <u>Forced Rethinking</u>, which appends a textual rethinking trigger to the end of initial rollouts in RL training, explicitly enforcing a self-reflection reasoning step. 
                </p>
                <div class="content has-text-centered">
                    <img src="static/images/overview.png" alt="algebraic reasoning" width="100%"/>
                </div>
                <p>
                    By combining these two techniques, our model, <u>VL-Rethinker, advances state-of-the-art scores on MathVista, MathVerse, and MathVision</u> to achieve significantly to achieve 80.3\%, 61.8\% and 43.9\% respectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary benchmarks such as MMMU-Pro, EMMA, and MEGA-Bench, narrowing the gap with GPT-o1. Our empirical results show the effectiveness of our approaches.
                </p>
              </div>
            </div>
          </div>
  
          <!-- <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Best-of-N Results</h2>
              <div class="content has-text-justified">
                <p>
                    We trained two reward model <a href="https://huggingface.co/TIGER-Lab/AceCodeRM-7B">AceCodeRM-7B</a> and <a href="https://huggingface.co/TIGER-Lab/AceCodeRM-32B">AceCodeRM-32B</a> on the constructed <a href="https://huggingface.co/datasets/TIGER-Lab/AceCodePair-300K">preference pairs</a>. We evaluate the performance of our reward models through the best-of-N experiments on the 4 popular coding benchmarks. Results show consistent improvement across all benchmarks, demonstrating the effectiveness of our reward models.
                </p>
                <div class="box m-5">
                    <div class="content has-text-centered">
                      <img src="static/images/ac_table2.png" alt="main best-of-N results" class="center" width="80%"/>
                    </div>
                  </div>
              </div>
            </div>
          </div>


          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">RL Results</h2>
              <div class="content has-text-justified">
                <p>
                    We perform RL training from three policy models: <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct</a> and <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B">Qwen2.5-Coder-7B-Base</a> and <a href="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct">Qwen2.5-Coder-7B-Instruct</a>. Two types of reward can be used, i.e. the trained reward model RM-7B and the rule-based reward, i.e. pass rate over the test cases in dataset. During training, we set the pass rate to be a binary reward, which is 1.0 when all test cases passed, otherwise 0. Similar to DeepSeek-R1, we also experiment with RL from the base model because SFT may cause the search space of the model to be stuck in the local minimum. Since coding is also a highly verifiable task like math, we include the Qwen2.5-Coder-7B-Base in our experiments. We see consisteny performance improvement across all benchmarks. And directly RL from the Base Qwen2.5-Coder model can get <b>25%</b> improvement on HumanEval-plus and <b>6%</b> on MBPP-plus within just <b>80</b> optimization steps.
                </p>
                <div class="box m-5">
                    <div class="content has-text-centered">
                      <img src="static/images/ac_table3.png" alt="RL results" class="center" width="80%"/>
                    </div>
                  </div>
              </div>
            </div>
          </div>

          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Comparison with existing RM</h2>
              <div class="content has-text-justified">
                <p>
                  Existing top-ranked reward models on Reward Bench can perform pretty bad for best-of-N sampling in the coding scenarion, and sometime can underperform the greedy results. However, our AceCodeRM-7B consistently outperform them with an average of <b>6.9</b> improvement 
                </p>
                <div class="box m-5">
                    <div class="content has-text-centered">
                      <img src="static/images/ac_table4.png" alt="Comparion with other RM" class="center" width="80%"/>
                    </div>
                  </div>
              </div>
            </div>
          </div>

          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Test case filtering matters</h2>
              <div class="content has-text-justified">
                <p>
                  We also conduct experiments to investigate how filtering the test cases with a proxy model can affect the results. As shown in table, training RM on data after the filtering improve the performance significantly, especially for those hard code questions like MBPP-Plus and BigCodeBench-Hard (C/I). We believe this is because the test case filtering can ensure the remaining ones are consistent with each other and thus point to the same implicit program, which improves the quality of the rewards. 
                </p>
                <div class="box m-5">
                    <div class="content has-text-centered">
                      <img src="static/images/ac_table5.png" alt="Test case filtering matters" class="center" width="80%"/>
                    </div>
                  </div>
              </div>
            </div>
          </div> -->

          <!-- <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">RM backbone matters</h2>
              <div class="content has-text-justified">
                <p>
                  We show that Qwen2.5-Coder is a better backbone for the reward model compared to Llama-3.1-8B. This is because the Qwen2.5-Coder models have been pre-trained on way more code-related data compared to the Llama-3.1 models, and thus more knowledgeable when tuning it into a reward model.
                </p>
                <div class="box m-5">
                    <div class="content has-text-centered">
                      <img src="static/images/ac_table6.png" alt="RM Backbone Matters" class="center" width="80%"/>
                    </div>
                  </div>
              </div>
            </div>
          </div> -->

        </div>
      </section>

	<!-- BibTeX citation -->
	<section class="section" id="BibTeX">
	    <div class="container is-max-desktop content">
	        <h2 class="title">Reference</h2>
	        Please kindly cite our paper if you use our code or results:
	        <pre><code>
			@article{vl-rethinker,
			      title={VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning},
			      author = {Wang, Haozhe and Qu, Chao and Huang, Zuming and Chu, Wei and Lin, Fangzhen and Chen, Wenhu},
			      journal={arXiv preprint arXiv:2504.08837},
			      year={2025}
			}
	        </code></pre>
	    </div>
	</section>

	<footer class="footer">
	    <div class="container">
	        <div class="columns is-centered">
	            <div class="column is-8">
	                <div class="content has-text-centered">
	                    <p>
	                        This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
	                    </p>
	                </div>
	            </div>
	        </div>
	    </div>
	</footer>

</body>
</html>
